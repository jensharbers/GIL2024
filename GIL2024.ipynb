{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f0f1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the versions of different libraries being used\n",
    "import scipy as sc\n",
    "import statsmodels as st\n",
    "import mlxtend as ml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Version of Pandas: \" + pd.__version__)\n",
    "print(\"Version of scipy: \" + sc.__version__)\n",
    "print(\"Version of statsmodels: \" + st.__version__)\n",
    "print(\"Version of mlxtend: \" + ml.__version__)\n",
    "print(\"Version of numpy: \" + np.__version__)\n",
    "\n",
    "\n",
    "from mlxtend.frequent_patterns import fpgrowth, association_rules\n",
    "import scipy.stats as stats\n",
    "from statsmodels.stats import multitest as mt\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864a20ba",
   "metadata": {},
   "source": [
    "\n",
    "# List of Abbreviations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8701e3ff",
   "metadata": {},
   "source": [
    "| Latin Abbreviation | Full Written Latin Species Name        |\n",
    "|--------------------|---------------------------------------|\n",
    "| DacGlom            | Dactylis glomerata                   |\n",
    "| PlaLanc            | Plantago lanceolata                  |\n",
    "| MedLupu            | Medicago lupulina                    |\n",
    "| GalAlbu            | Galium album                         |\n",
    "| TriPrat            | Trifolium pratensis                  |\n",
    "| PoaTriv            | Poa trivialis                        |\n",
    "| TarRude            | Taraxacum sect. Ruderali             |\n",
    "| LeuIrcu            | Leucanthemum ircutianum              |\n",
    "| RanBulb            | Ranunculus bulbosus                  |\n",
    "| AntOdor            | Anthoxanthum odoratum                |\n",
    "| FesRubr            | Festuca rubra                        |\n",
    "| CerHolo            | Cerastium holosteoides               |\n",
    "| CerGlom            | Cerastium glomeratum                 |\n",
    "| ArrElat            | Arrhenatherum elatius                |\n",
    "| KnaArve            | Knautia arvensis                     |\n",
    "| LotCorn            | Lotus corniculatus                   |\n",
    "| RosSpec            | Rosa specie                          |\n",
    "| SanMino            | Sanguisorba minor                    |\n",
    "| TriDubi            | Trifolium dubium                     |\n",
    "| PriVeri            | Primula veris                        |\n",
    "| VicAngu            | Vicia angustifolia                   |\n",
    "| VioHirt            | Viola hirta                          |\n",
    "| DauCaro            | Daucus carota                        |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7806263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getrules(file,separator,correction_methods,field, minsupport, min_threshold, significant_pvalue, metric, n_spec,n_rules):\n",
    "\n",
    "    df = pd.read_csv(file.name,sep=separator,  encoding='utf-8')\n",
    "    \n",
    "    remove_species = [\"FesPrat\", \"LolPere\", \"PhlPrat\", \"PoaTriv\", \"BroErec\", \"TriFlav\", \"PlaLanc\"]\n",
    "    n = df.area.nunique()\n",
    "    cnt = df.variable.value_counts() / n\n",
    "    df = df[~df.variable.isin(remove_species)]\n",
    "\n",
    "    # Get the list of species present in the field \"A10_16\"\n",
    "    species_on_field = df[df.area == field].variable.values\n",
    "\n",
    "    # Remove species from the DataFrame that occur less frequently (below a threshold) in the dataset\n",
    "    df = df[~df.variable.isin(cnt[(cnt < minsupport)].index.values.tolist())]\n",
    "\n",
    "    # Create a cross-tabulation between 'area' and 'variable' to represent the dataset as a binary matrix\n",
    "    df = pd.crosstab(df.area, df.variable)\n",
    "\n",
    "    # Apply frequent pattern growth algorithm to find frequent itemsets\n",
    "    frequent_itemsets = fpgrowth(df, min_support=minsupport, use_colnames=True, max_len=20)\n",
    "\n",
    "    # Apply association rule mining to generate association rules\n",
    "    assoc = association_rules(frequent_itemsets, metric=metric, min_threshold=min_threshold, support_only=False)\n",
    "\n",
    "    # Filter the rules: Keep only those where all species in the antecedents are present in the field\n",
    "    assoc[\"antecedent_in_field\"] = assoc.apply(lambda x: x.antecedents.issubset(species_on_field), axis=1)\n",
    "    assoc = assoc[assoc.antecedent_in_field == True]\n",
    "\n",
    "    # Filter the rules further: Keep only those where no species from the 'remove_cons' list is present in the consequents\n",
    "    remove_cons = species_on_field\n",
    "    assoc[\"consequent_not_in_field\"] = assoc.apply(lambda x: x.consequents.isdisjoint(remove_cons), axis=1)\n",
    "    assoc = assoc[assoc.consequent_not_in_field == True]\n",
    "\n",
    "    # Perform additional calculations on the DataFrame to prepare for statistical tests\n",
    "    assoc[\"size_antecedents\"] = assoc.apply(lambda x: len(x.antecedents), axis=1)\n",
    "    assoc[\"size_consequents\"] = assoc.apply(lambda x: len(x.consequents), axis=1)\n",
    "    assoc['n1x'] = assoc['antecedent support'] * n\n",
    "    assoc['nx1'] = assoc['consequent support'] * n\n",
    "    assoc['n11'] = assoc['support'] * n\n",
    "    assoc['n0x'] = n - assoc['n1x']\n",
    "    assoc['nx0'] = n - assoc['nx1']\n",
    "    assoc['n10'] = assoc[\"n1x\"] - assoc['n11']\n",
    "    assoc['n01'] = assoc[\"nx1\"] - assoc['n11']\n",
    "    assoc['n00'] = assoc['n0x'] - assoc['n01']\n",
    "\n",
    "    # Reduce calculations of Fisher's exact test to speed up the process\n",
    "    df2 = assoc[[\"n11\", \"n10\", \"n01\", \"n00\"]].drop_duplicates().copy()\n",
    "    df2[\"pval\"] = df2.apply(lambda x: stats.fisher_exact([[x.n11, x.n10], [x.n01, x.n00]], \"two-sided\")[1], axis=1)\n",
    "    assoc = assoc.merge(df2)\n",
    "\n",
    "    # Perform multiple testing correction on the p-values using various methods\n",
    "    assoc.iloc[:, 9:19] = assoc.iloc[:, 9:19].astype(int)\n",
    "    assoc = assoc.sort_values(\"pval\")\n",
    "\n",
    "    assoc[\"p_corr\"] = mt.multipletests(assoc.pval, significant_pvalue, method=\"fdr_bh\", is_sorted=True)[1]\n",
    "\n",
    "    # Calculate the expected number of species in the consequents based on confidence\n",
    "    assoc[\"expected_species\"] = assoc[\"size_consequents\"] * assoc[\"confidence\"]\n",
    "\n",
    "    # Print the final dimensions of the DataFrame after all calculations\n",
    "    print(f\"Datensatzdimension nach allen Berechnungen: {assoc.shape}\")\n",
    "\n",
    "    # Save the results to a CSV file\n",
    "    # assoc.to_csv(\"Saatgutmischungen_10_16_schedenveg_0.12S_0.2C.csv\", index=False)\n",
    "\n",
    "    # Select the significant rules based on adjusted p-value and other criteria, and save them to a CSV file\n",
    "    significant = assoc[(assoc.p_corr < significant_pvalue) & (assoc.expected_species > n_spec)].sort_values(\n",
    "        [\"expected_species\", \"p_corr\", \"size_antecedents\"], ascending=[False, False, True]\n",
    "    )\n",
    "    significant = significant.groupby(\"consequents\").first()\n",
    "    significant = significant.sort_values(\n",
    "        [\"expected_species\", \"p_corr\", \"size_antecedents\"], ascending=[False, False, True]\n",
    "    ).reset_index()\n",
    "\n",
    "    # Print the number of significant rules after correction\n",
    "    print(f\"Anzahl aller signifikanten p-Wert korrigierten Regeln: {significant.shape[0]}\")\n",
    "\n",
    "    significant[[\"support\",\"confidence\",\"expected_species\",\"pval\",\"p_corr\"]] = significant[[\"support\",\"confidence\",\"expected_species\",\"pval\",\"p_corr\"]].apply(lambda x:np.round(x,4))\n",
    "\n",
    "    # Select the top 10 significant rules and perform set union operation on their antecedents and consequents\n",
    "    cp = significant[[\"antecedents\", \"consequents\", \"support\", \"confidence\", \"pval\", \"p_corr\", \"expected_species\", \"lift\"]][:int(n_rules)].copy()\n",
    "    return cp\n",
    "\n",
    "\n",
    "# Create Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=getrules,\n",
    "    inputs=[\n",
    "        gr.File(type=\"file\",file_types=[\".csv\"], file_count=\"single\"),\n",
    "        gr.Text(label=\"Separator\", value=\" \"),\n",
    "        gr.Dropdown(\n",
    "            choices=[\"bonferroni\", \"sidak\", \"holm\", \"holm-sidak\", \"simes-hochberg\", \"fdr_bh\", \"fdr_tsbh\", \"fdr_by\", \"fdr_tsbky\", \"hommel\"],\n",
    "            label=\"Choose a correction method\",\n",
    "            value=\"fdr_bh\"\n",
    "        ),\n",
    "        gr.Text(\n",
    "            label=\"Select a field\",\n",
    "            value=\"A10_16\"\n",
    "        ),\n",
    "        gr.Number(\n",
    "            label=\"Minimum Support\",\n",
    "            value=0.12,\n",
    "            step=0.01\n",
    "        ),\n",
    "        gr.Number(\n",
    "            label=\"Minimum Threshold of Metric\",\n",
    "            value=0.20,\n",
    "            step=0.01\n",
    "        ),\n",
    "        gr.Number(\n",
    "            label=\"Significant P-Value\",\n",
    "            value=0.05,\n",
    "            step=0.01\n",
    "        ),\n",
    "        gr.Dropdown(\n",
    "            choices=[\"support\",\"lift\",\"confidence\",\"leverage\",\"conviction\",\"zhangs_metric\"],\n",
    "            label=\"Metric\",\n",
    "            value=\"confidence\"\n",
    "        ),\n",
    "        gr.Number(\n",
    "            label=\"Least Expected Number of Species\",\n",
    "            value=2,\n",
    "            step=1\n",
    "        ),\n",
    "        gr.Number(\n",
    "            label=\"Number of returned significant rules\",\n",
    "            value=10,\n",
    "            step=1\n",
    "        )\n",
    "    ],\n",
    "    outputs=\"dataframe\",\n",
    "    live=False,\n",
    "    title=\"Association Rule Mining of Grassland with p-value correction\",\n",
    "    description=\"Find association rules with the specified parameters.\",\n",
    "    allow_flagging='never'\n",
    ")\n",
    "\n",
    "# Launch the Gradio interface\n",
    "iface.launch(debug=True, server_name=\"localhost\", server_port=7862)\n",
    "\n",
    "# This code sets up the Gradio interface with an output function called download_rules that saves the DataFrame to a CSV file and returns the file path. Users can click the \"Download CSV\" button to download the generated rules as a CSV file. Make sure to replace 'your_module' with the actual name of the module where your getrules function is defined.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:newestenvir]",
   "language": "python",
   "name": "conda-env-newestenvir-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
