{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb9b534",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mlxtend.frequent_patterns import fpgrowth, association_rules\n",
    "import scipy.stats as stats\n",
    "from statsmodels.stats import multitest as mt\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\Jens Harbers\\\\Documents\\\\schedenveg_reproduce.csv\", sep=\" \")\n",
    "\n",
    "# Manually remove some species based on their abundance in the dataset\n",
    "# 'n' represents the number of unique areas\n",
    "n = df.area.nunique()\n",
    "cnt = df.variable.value_counts() / n\n",
    "\n",
    "# For those wanting to remove additional species (Big 4 species and some quite abundant ones):\n",
    "remove_species = [\"FesPrat\", \"LolPere\", \"PhlPrat\", \"PoaTriv\", \"BroErec\", \"TriFlav\", \"PlaLanc\"]\n",
    "df = df[~df.variable.isin(remove_species)]\n",
    "\n",
    "# Get the list of species present in the field \"A10_16\"\n",
    "species_on_field = df[df.area == \"A10_16\"].variable.values\n",
    "\n",
    "# Remove species from the DataFrame that occur less frequently (below a threshold) in the dataset\n",
    "df = df[~df.variable.isin(cnt[(cnt <= 0.12)].index.values.tolist())]\n",
    "\n",
    "# Create a cross-tabulation between 'area' and 'variable' to represent the dataset as a binary matrix\n",
    "df = pd.crosstab(df.area, df.variable)\n",
    "\n",
    "# Apply frequent pattern growth algorithm to find frequent itemsets\n",
    "frequent_itemsets = fpgrowth(df, min_support=0.12, use_colnames=True, max_len=20)\n",
    "\n",
    "# Apply association rule mining to generate association rules\n",
    "assoc = association_rules(frequent_itemsets, metric='confidence', min_threshold=0.20, support_only=False)\n",
    "\n",
    "# Filter the rules: Keep only those where all species in the antecedents are present in the field\n",
    "assoc[\"antecedent_in_field\"] = assoc.apply(lambda x: x.antecedents.issubset(species_on_field), axis=1)\n",
    "assoc = assoc[assoc.antecedent_in_field == True]\n",
    "\n",
    "# Filter the rules further: Keep only those where no species from the 'remove_cons' list is present in the consequents\n",
    "remove_cons = np.append(species_on_field, [\"RumAcet\", \"DacGlom\", \"FesPrat\", \"LolPere\", \"PhlPrat\", \"PoaTriv\", \"PoaPrat\"])\n",
    "assoc[\"consequent_not_in_field\"] = assoc.apply(lambda x: x.consequents.isdisjoint(remove_cons), axis=1)\n",
    "assoc = assoc[assoc.consequent_not_in_field == True]\n",
    "\n",
    "# Perform additional calculations on the DataFrame to prepare for statistical tests\n",
    "assoc[\"size_antecedents\"] = assoc.apply(lambda x: len(x.antecedents), axis=1)\n",
    "assoc[\"size_consequents\"] = assoc.apply(lambda x: len(x.consequents), axis=1)\n",
    "assoc['n1x'] = assoc['antecedent support'] * n\n",
    "assoc['nx1'] = assoc['consequent support'] * n\n",
    "assoc['n11'] = assoc['support'] * n\n",
    "assoc['n0x'] = n - assoc['n1x']\n",
    "assoc['nx0'] = n - assoc['nx1']\n",
    "assoc['n10'] = assoc[\"n1x\"] - assoc['n11']\n",
    "assoc['n01'] = assoc[\"nx1\"] - assoc['n11']\n",
    "assoc['n00'] = assoc['n0x'] - assoc['n01']\n",
    "\n",
    "# Reduce calculations of Fisher's exact test to speed up the process\n",
    "df2 = assoc[[\"n11\", \"n10\", \"n01\", \"n00\"]].drop_duplicates().copy()\n",
    "df2[\"pval\"] = df2.apply(lambda x: stats.fisher_exact([[x.n11, x.n10], [x.n01, x.n00]], \"greater\")[1], axis=1)\n",
    "assoc = assoc.merge(df2)\n",
    "\n",
    "# Perform multiple testing correction on the p-values using various methods\n",
    "assoc.iloc[:, 9:19] = assoc.iloc[:, 9:19].astype(int)\n",
    "assoc = assoc.sort_values(\"pval\")\n",
    "assoc[\"bonferroni\"] = mt.multipletests(assoc.pval, 0.05, method=\"bonferroni\", is_sorted=True)[1]\n",
    "assoc[\"sidak\"] = mt.multipletests(assoc.pval, 0.05, method=\"sidak\", is_sorted=True)[1]\n",
    "assoc[\"holm\"] = mt.multipletests(assoc.pval, 0.05, method=\"holm\", is_sorted=True)[1]\n",
    "assoc[\"holm_sidak\"] = mt.multipletests(assoc.pval, 0.05, method=\"holm-sidak\", is_sorted=True)[1]\n",
    "assoc[\"simes_hochberg\"] = mt.multipletests(assoc.pval, 0.05, method=\"simes-hochberg\", is_sorted=True)[1]\n",
    "assoc[\"fdr_bh\"] = mt.multipletests(assoc.pval, 0.05, method=\"fdr_bh\", is_sorted=True)[1]\n",
    "assoc[\"fdr_tsbh\"] = mt.multipletests(assoc.pval, 0.05, method=\"fdr_tsbh\", is_sorted=True)[1]\n",
    "assoc[\"fdr_by\"] = mt.multipletests(assoc.pval, 0.05, method=\"fdr_by\", is_sorted=True)[1]\n",
    "assoc[\"fdr_tsbky\"] = mt.multipletests(assoc.pval, 0.05, method=\"fdr_tsbky\", is_sorted=True)[1]\n",
    "\n",
    "# Calculate the expected number of species in the consequents based on confidence\n",
    "assoc[\"expected_species\"] = assoc[\"size_consequents\"] * assoc[\"confidence\"]\n",
    "\n",
    "# Print the final dimensions of the DataFrame after all calculations\n",
    "print(f\"Datensatzdimension nach allen Berechnungen: {assoc.shape}\")\n",
    "\n",
    "# Print the versions of different libraries being used\n",
    "import scipy as sc\n",
    "import statsmodels as st\n",
    "import mlxtend as ml\n",
    "\n",
    "print(\"Version of Pandas: \" + pd.__version__)\n",
    "print(\"Version of numPy: \" + np.__version__)\n",
    "print(\"Version of scipy: \" + sc.__version__)\n",
    "print(\"Version of statsmodels: \" + st.__version__)\n",
    "print(\"Version of mlxtend: \" + ml.__version__)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "assoc.to_csv(\"Saatgutmischungen_10_16_schedenveg_0.12S_0.2C.csv\", index=False)\n",
    "\n",
    "# Select the significant rules based on adjusted p-value and other criteria, and save them to a CSV file\n",
    "significant = assoc[(assoc.fdr_bh < 0.05) & (assoc.expected_species > 2)].sort_values(\n",
    "    [\"expected_species\", \"fdr_bh\", \"size_antecedents\"], ascending=[False, False, True]\n",
    ")\n",
    "significant = significant.groupby(\"consequents\").first()\n",
    "significant = significant.sort_values(\n",
    "    [\"expected_species\", \"fdr_bh\", \"size_antecedents\"], ascending=[False, False, True]\n",
    ").reset_index()\n",
    "\n",
    "# Print the number of significant rules after correction\n",
    "print(f\"Anzahl aller signifikanten nach fdr_bh korrigierten Regeln: {significant.shape[0]}\")\n",
    "\n",
    "# Select the top 10 significant rules and perform set union operation on their antecedents and consequents\n",
    "cp = significant[[\"antecedents\", \"consequents\", \"support\", \"confidence\", \"pval\", \"fdr_bh\", \"expected_species\", \"lift\"]][:10].copy()\n",
    "(\n",
    "    cp.antecedents.values[0] | cp.antecedents.values[1] | cp.antecedents.values[2] | cp.antecedents.values[3] |\n",
    "    cp.antecedents.values[4] | cp.antecedents.values[5] | cp.antecedents.values[6] | cp.antecedents.values[7] |\n",
    "    cp.antecedents.values[8] | cp.antecedents.values[9]\n",
    ")\n",
    "(\n",
    "    cp.consequents.values[0] | cp.consequents.values[1] | cp.consequents.values[2] | cp.consequents.values[3] |\n",
    "    cp.consequents.values[4] | cp.consequents.values[5] | cp.consequents.values[6] | cp.consequents.values[7] |\n",
    "    cp.consequents.values[8] | cp.consequents.values[9]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864a20ba",
   "metadata": {},
   "source": [
    "\n",
    "# List of Abbreviations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8701e3ff",
   "metadata": {},
   "source": [
    "| Latin Abbreviation | Full Written Latin Species Name        |\n",
    "|--------------------|---------------------------------------|\n",
    "| DacGlom            | Dactylis glomerata                   |\n",
    "| PlaLanc            | Plantago lanceolata                  |\n",
    "| MedLupu            | Medicago lupulina                    |\n",
    "| GalAlbu            | Galium album                         |\n",
    "| TriPrat            | Trifolium pratensis                  |\n",
    "| PoaTriv            | Poa trivialis                        |\n",
    "| TarRude            | Taraxacum sect. Ruderali             |\n",
    "| LeuIrcu            | Leucanthemum ircutianum              |\n",
    "| RanBulb            | Ranunculus bulbosus                  |\n",
    "| AntOdor            | Anthoxanthum odoratum                |\n",
    "| FesRubr            | Festuca rubra                        |\n",
    "| CerHolo            | Cerastium holosteoides               |\n",
    "| CerGlom            | Cerastium glomeratum                 |\n",
    "| ArrElat            | Arrhenatherum elatius                |\n",
    "| KnaArve            | Knautia arvensis                     |\n",
    "| LotCorn            | Lotus corniculatus                   |\n",
    "| RosSpec            | Rosa specie                          |\n",
    "| SanMino            | Sanguisorba minor                    |\n",
    "| TriDubi            | Trifolium dubium                     |\n",
    "| PriVeri            | Primula veris                        |\n",
    "| VicAngu            | Vicia angustifolia                   |\n",
    "| VioHirt            | Viola hirta                          |\n",
    "| DauCaro            | Daucus carota                        |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:newestenvir]",
   "language": "python",
   "name": "conda-env-newestenvir-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
